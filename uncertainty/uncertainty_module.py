import numpy as np
import torch
from tqdm import tqdm
from torchvision.transforms import v2

def get_uncertainty(dataloader, model, num_points=10, order='top-k', device='cuda:0'):
    unc_scores = []
    for samples, _ in tqdm(dataloader):
        unc_batch = compute_uncertainty_scores(samples, model, device)
        unc_scores.extend(unc_batch)
    if order == 'top-k':
        score_idx = torch.sort(torch.tensor(unc_scores), descending=True)[1][:num_points]
    elif order == 'bottom-k':
        score_idx = torch.sort(torch.tensor(unc_scores), descending=False)[1][:num_points]
    else:
        raise ValueError(f"Invalid value for 'order': {order}. Expected 'top-k' or 'bottom-k' for the 'order' parameter.")
    return unc_scores, score_idx


def compute_uncertainty_scores(input_x, model, device):
    if len(input_x.shape) > 3:  # (batch_size, channels, image_size, image_size)
        logits_out = get_image_logits(input_x, model, device=device)
    else:                       # (batch_size, embedding_size)
        logits_out = get_text_logits(input_x, model, device=device)

    unc_scores = BI_LSE(logits_out)
    return unc_scores


@torch.no_grad()
def get_text_logits(input_x, model, tta_rep=5, device='cuda:0'):
    # evaluate prediction on the augmented textual embeddings
    # and store the corresponding logits
    input_x = input_x.to(device)
    with torch.no_grad():
        model.eval()
        all_logits = []
        for rep in range(tta_rep):
            bx_tmp = add_gaussian_noise(input_x)
            logits_tmp = model(bx_tmp)
            all_logits.append(logits_tmp)
    
    logits_out = torch.stack(all_logits).detach().cpu()
    return logits_out


@torch.no_grad()
def get_image_logits(input_x, model, device='cuda:0'):
    # evaluate prediction on the augmented images given the sequence of transform functions
    # and store the corresponding logits
    transform_cands = [
        v2.RandomHorizontalFlip(),
        v2.RandomVerticalFlip(),
        v2.RandomRotation(degrees=10),
        v2.RandomRotation(45),
        v2.RandomRotation(90),
        v2.ColorJitter(brightness=0.1),
        v2.RandomPerspective(),
        v2.RandomAffine(degrees=20, translate=(0.1, 0.3), scale=(0.5, 0.75)),
        v2.RandomResizedCrop([3,32,32][1:], scale=(0.8, 1.0), ratio=(0.9, 1.1), antialias=True),
        v2.RandomInvert()
            ]

    input_x = input_x.to(device)
    all_logits = []
    for tr in transform_cands:
        bx_tmp = tr(input_x)
        logits_tmp = model(bx_tmp)
        all_logits.append(logits_tmp)

    logits_out = torch.stack(all_logits).detach().cpu()
    return logits_out


def add_gaussian_noise(embedding_matrix, mean=0, std=0.1, device='cuda:0'):
    noise = torch.normal(mean=mean, std=std, size=embedding_matrix.shape).to(device)
    noisy_embedding_matrix = embedding_matrix + noise
    return noisy_embedding_matrix


# taken from https://github.com/MLO-lab/Uncertainty_Estimates_via_BVD
def BI_LSE(zs, axis=0, class_axis=-1):
    '''
    Bregman Information of random variable Z generated by G = LSE
    BI_G [ Z ] = E[ G( Z ) ] - G( E[ Z ] )
    We estimate with dataset zs = [Z_1, ..., Z_n] via
    1/n sum_i G( Z_i ) - G( 1/n sum_i Z_i )
    
    Arg zs: Tensor with shape length >= 2
    Arg axis: Axis of the samples to average over
    Arg class_axis: Axis of the class logits
    Output: Tensor with shape length reduced by two
    '''
    if not isinstance(zs, torch.Tensor):
        raise TypeError(f"'zs' must be a PyTorch tensor, but got {type(zs)}")
    
    E_of_LSE = zs.logsumexp(axis=class_axis).mean(axis)
    LSE_of_E = zs.mean(axis).unsqueeze(axis).logsumexp(axis=class_axis).squeeze(axis)
    bi_scores = E_of_LSE - LSE_of_E
    return bi_scores